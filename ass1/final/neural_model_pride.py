# -*- coding: utf-8 -*-
"""neural_model_pride.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1irHf0p7HDzZ40cHJZHZ1szz6ZDMzCkYd
"""

# Colab stuff
# from google.colab import drive
# drive.mount('/content/drive')
# !cp /content/drive/MyDrive/preprocessed_data.txt a.txt

import regex as re
from keras.layers import Embedding, LSTM, Dense
from keras.models import Sequential
import numpy as np
f = open("a.txt", "r")
data = f.read()

arr = re.findall("<SOS>(.*?)<EOS>", data)
maxlen = max([len(seq) for seq in arr])

print(maxlen)


np.random.seed(41)
train_split=[]
test_split=[]
dev_split=[]
test_size=int(round(0.15*len(arr)))
select_id=np.random.choice(len(arr),test_size,replace=True)

for i,val in enumerate(arr):

    if i in select_id:
        test_split.append(val.strip())

    else:
        
        x="<SOS> "
        x+=val.strip()
        x+=" <EOS>"
        train_split.append(x)

train_split=" ".join(train_split)
arr = re.findall("<SOS>(.*?)<EOS>", train_split)
train_split=[]
select_id=np.random.choice(len(arr),test_size,replace=True)
for i,val in enumerate(arr):
    if i in select_id:
        dev_split.append(val.strip())
    else:
        x="<SOS> "
        x+=val.strip()
        x+=" <EOS>"
        train_split.append(x)

sent_arr=" ".join(train_split)

print(sent_arr)

"""### Create unigram dictionary and vocablury building"""

vocab_dict={}
sent_arr = re.findall("<SOS>(.*?)<EOS>", data)

i=0
for sent in sent_arr:
  x=re.split(" ",sent)
  x = list(filter(None, x))

  for word in x:
    if word != None:
      if(word in vocab_dict):
        vocab_dict[word]+=1
      else:
        vocab_dict[word]=1
vocab_dict['<UNK>']=0
for key in list(vocab_dict):
  if(vocab_dict[key]<2):
    vocab_dict['<UNK>']+=vocab_dict[key]
    del vocab_dict[key]

# print(len(vocab_dict))
# print(vocab_dict)
# print(vocab_dict['<UNK>'])
# print(list(vocab_dict))

# Convert to vocablury now
lst=list(vocab_dict)
final_dict = {lst[i]: i+1 for i in range(0, len(lst), 1)}
print(final_dict)

"""### Model Sentences to the assigned vector"""

tokenised_sequence=[]
for sent in sent_arr:
  x=re.split(" ",sent)
  x = list(filter(None, x))
  suplmentary_arr=[]
  for word in x:
    try:
      suplmentary_arr.append(final_dict[word])
    except:
      suplmentary_arr.append(final_dict['<UNK>'])

  tokenised_sequence.append(suplmentary_arr)

# print(tokenised_sequence)

"""Convert these sequences to vectors to feed them in LSTM"""

def prepare_sentence(seq, maxlen):
    # Pads seq and slides windows
    x = []
    y = []
    for i, w in enumerate(seq):
          # print(i)
          # print([seq[:i]])
          x_padded_new=[0]*(maxlen-1-i)
          x_padded_new+=seq[:i]
          x.append(x_padded_new)
          y.append(w)
    
    return x, y


def convert_sequencer():
  x = []
  y = []
  for seq in tokenised_sequence:
    x_windows, y_windows = prepare_sentence(seq, maxlen)
    x += x_windows
    y += y_windows
  return x,y

x,y=convert_sequencer()

x = np.array(x)
y = np.array(y) - 1

y = np.eye(len(vocab_dict))[y]  # One hot encoding

"""## Train Model"""

# # uncomment this to train model
# model = Sequential()
# model.add(Embedding(input_dim=len(vocab_dict) + 1,  # vocabulary size. Adding an
#                                                # extra element for <PAD> word
#                     output_dim=5,  # size of embeddings
#                     input_length=maxlen - 1))  # length of the padded sequences
# model.add(LSTM(128,recurrent_dropout=0.5))
# model.add(Dense(len(vocab_dict), activation='softmax'))
# model.compile('rmsprop', 'categorical_crossentropy')

# # # Train network
# model.fit(x, y, epochs=4)

# !rm -rf "/content/model_name.h5"
# model.save('/content/model_name.h5')
# !cp /content/drive/MyDrive/final_model_name.h5 .

"""## Prediction """

from keras.models import load_model

model = load_model('/content/drive/MyDrive/final_model_name.h5')

def special_cases(wiki):
    wiki=re.sub("_"," ",wiki)
    wiki=re.sub(r'"',"",wiki)
    # wiki = re.sub(r'\s+\'bout', r' about', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'t', r'\1 not', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'s', r'\1 is', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'re', r'\1 are', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'ll', r'\1 will', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'d', r'\1 would', wiki)
    wiki = re.sub(r'([a-zA-Z]+)\'ve', r'\1 have', wiki)
    wiki = re.sub(r'([iI])\'m', r'\1 am', wiki)
    wiki=re.sub(r"won\'t","will not",wiki)
    wiki=re.sub(r"can\'t","cannot",wiki)

    return wiki

def tokenizer(wiki):

    wiki = re.sub(r'[^\x00-\x7F]+', ' ', wiki)
    prefix=['@','#']
    # for sep in string.
    wiki = re.sub(r"@[A-Za-z0-9_]+", "<MENTION>", wiki) # mention
    wiki = re.sub(r"#[A-Za-z0-9_]+", "<HASHTAG>", wiki) # hashtag
    wiki = re.sub(r'(https?:\/\/|www\.)?\S+[a-zA-Z0-9]{2,}\.[a-zA-Z0-9]{2,}\S+', ' <URL> ', wiki)  #url
    return wiki

def remove_stupid_fullstop(wiki):
    wiki=re.sub("Mr\s*.","Mr",wiki)
    wiki=re.sub("Ms\s*.","Ms",wiki)
    wiki=re.sub("Mrs\s*.","Mrs",wiki)
    wiki=re.sub("Miss\s*.","Miss",wiki)

    return wiki

def detect_full_stop(data):
    # data = re.sub(r'!+', "!", data)
    # data = re.sub(r'\?+', "?", data)
    data=data.lower()
    data = re.sub(r'[^\w+^\.^\?^\!\s]', r' ', data)
    data=re.sub(r'(\.+)'," <EOS> <SOS> ",data)
    return data

def convert_sent_arr(sentence):
    x=re.split(" ",sentence)
    x = list(filter(None, x))
    suplmentary_arr=[]
    for word in x:
      try:
        suplmentary_arr.append(final_dict[word])
      except:
        suplmentary_arr.append(final_dict['<UNK>'])
    return suplmentary_arr

def calculate_probability(sentence):
  tok=convert_sent_arr(sentence)
  x_test, y_test = prepare_sentence(tok, maxlen)
  x_test = np.array(x_test)
  y_test = np.array(y_test) - 1  # The word <PAD> does not have a class
  p_pred = model.predict(x_test)
  vocab_inv = {v: k for k, v in final_dict.items()}
  log_p_sentence = 0
  for i, prob in enumerate(p_pred):
      word = vocab_inv[y_test[i]+1]  # Index 0 from vocab is reserved to <PAD>
      history = ' '.join([vocab_inv[w] for w in x_test[i, :] if w != 0])
      prob_word = prob[y_test[i]]
      log_p_sentence += np.log(prob_word)
  #     print('P(w={}|h={})={}'.format(word, history, prob_word))
  # print('Prob. sentence: {}'.format(np.exp(log_p_sentence)))

  return (np.exp(log_p_sentence))

def get_perplexity(text):
        return np.power(1/calculate_probability(text), 1/(len(text.split())))

calculate_probability("i am a women")

# avg=0
# smoothing="neural_model"
# filename="a.txt"
# out_path = "/content/"+ "2020115005"  +"_"+"test"  + "_" + smoothing +"_"+ filename
# f = open(out_path, "w")
# f=open(out_path,"a")
# for sent in test_split:
#   ref="<SOS> "
#   ref+=sent+" <EOS>"
#   ref=re.sub("/s+"," ",ref)
#   val=get_perplexity(ref)
#   avg+=val
#   print(val)
#   f.write(ref +" ");f.write(str(val)+"\n")
# f.write("Average Value: ")
# f.write(str(avg/len(test_split))+"\n")
# print("Average value: ")
# print(avg/len(test_split))

# avg=0
# smoothing="neural_model"
# filename="a.txt"
# out_path = "/content/"+ "2020115005"  +"_"+"train"  + "_" + smoothing +"_"+ filename
# f = open(out_path, "w")
# f=open(out_path,"a")
# for sent in train_split:
#   ref="<SOS> "
#   ref+=sent+" <EOS>"
#   ref=re.sub("/s+"," ",ref)
#   val=get_perplexity(ref)
#   avg+=val
#   print(val)
#   f.write(ref +" ");f.write(str(val)+"\n")
# f.write("Average Value: ")
# f.write(str(avg/len(train_split))+"\n")
# print("Average value: ")
# print(avg/len(train_split))