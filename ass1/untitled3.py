# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1irHf0p7HDzZ40cHJZHZ1szz6ZDMzCkYd
"""

# from google.colab import drive
#drive.mount('/content/drive')
#!cp /content/drive/MyDrive/preprocessed_data.txt a.txt

#!pip install keras_preprocessing
import regex as re
from keras_preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense
from keras.models import Sequential
import numpy as np
f = open("a.txt", "r")
data = f.read()

sent_arr = re.findall("<SOS>(.*?)<EOS>", data)

#print(sent_arr)

"""### Create unigram dictionary and vocablury building"""

vocab_dict={}
i=0
for sent in sent_arr:
  x=re.split(" ",sent)
  x = list(filter(None, x))

  for word in x:
    if word != None:
      if(word in vocab_dict):
        vocab_dict[word]+=1
      else:
        vocab_dict[word]=1
vocab_dict['<UNK>']=0
for key in list(vocab_dict):
  if(vocab_dict[key]<10):
    vocab_dict['<UNK>']+=vocab_dict[key]
    del vocab_dict[key]

#print(len(vocab_dict))
#print(vocab_dict)
#print(vocab_dict['<UNK>'])
# print(list(vocab_dict))

# Convert to vocablury now
lst=list(vocab_dict)
final_dict = {lst[i]: i+1 for i in range(0, len(lst), 1)}
#print(final_dict)

"""### Model Sentences to the assigned vector"""

tokenised_sequence=[]
for sent in sent_arr:
  x=re.split(" ",sent)
  x = list(filter(None, x))
  suplmentary_arr=[]
  for word in x:
    try:
      suplmentary_arr.append(final_dict[word])
    except:
      suplmentary_arr.append(final_dict['<UNK>'])

  tokenised_sequence.append(suplmentary_arr)

# print(tokenised_sequence)

"""Convert these sequences to vectors to feed them in LSTM"""

def prepare_sentence(seq, maxlen):
    # Pads seq and slides windows
    x = []
    y = []
    for i, w in enumerate(seq):
          x_padded = pad_sequences([seq[:i]],maxlen=maxlen - 1,padding='pre')[0]  # Pads before each sequence
          x.append(x_padded)
          y.append(w)
    
    return x, y

maxlen = max([len(seq) for seq in tokenised_sequence])
x = []
y = []
for seq in tokenised_sequence:
    x_windows, y_windows = prepare_sentence(seq, maxlen)
    x += x_windows
    y += y_windows

x = np.array(x)
y = np.array(y) - 1

y = np.eye(len(vocab_dict))[y]  # One hot encoding





# data = ["Two little dicky birds",
#         "Sat on a wall,",
#         "One called Peter,",
#         "One called Paul.",
#         "Fly away, Peter,",
#         "Fly away, Paul!",
#         "Come back, Peter,",
#         "Come back, Paul."]

# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(data)
# vocab = tokenizer.word_index
# print(vocab)
# seqs = tokenizer.texts_to_sequences(data)
# print(seqs)

model = Sequential()
model.add(Embedding(input_dim=len(vocab_dict) + 1,  # vocabulary size. Adding an
                                               # extra element for <PAD> word
                    output_dim=5,  # size of embeddings
                    input_length=maxlen - 1))  # length of the padded sequences
#model.add(LSTM(10))
model.add(LSTM(128,recurrent_dropout = 0.5))
model.add(Dense(len(vocab_dict), activation='softmax'))
model.compile('rmsprop', 'categorical_crossentropy')

# Train network
model.fit(x, y, epochs=4)
#!rm -rf "/content/model_name.h5"
model.save('/scratch/devesh.marwah/model_name.h5')

def convert_sent_arr(sentence):
    x=re.split(" ",sentence)
    x = list(filter(None, x))
    suplmentary_arr=[]
    for word in x:
      try:
        suplmentary_arr.append(final_dict[word])
      except:
        suplmentary_arr.append(final_dict['<UNK>'])
    return suplmentary_arr

sentence = "it is a truth universally acknowledged  that a single man in possession of a good fortune  must be in want of a wife "
# tok = tokenizer.texts_to_sequences([sentence])[0]
# print(tok)
tok=convert_sent_arr(sentence)
x_test, y_test = prepare_sentence(tok, maxlen)
x_test = np.array(x_test)
y_test = np.array(y_test) - 1  # The word <PAD> does not have a class
p_pred = model.predict(x_test)
vocab_inv = {v: k for k, v in final_dict.items()}
log_p_sentence = 0
for i, prob in enumerate(p_pred):
    word = vocab_inv[y_test[i]+1]  # Index 0 from vocab is reserved to <PAD>
    history = ' '.join([vocab_inv[w] for w in x_test[i, :] if w != 0])
    
    prob_word = prob[y_test[i]]
    log_p_sentence += np.log(prob_word)
    print('P(w={}|h={})={}'.format(word, history, prob_word))
print('Prob. sentence: {}'.format(np.exp(log_p_sentence)))
